{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from decouple import config\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env file at: c:\\Users\\Frede\\Code\\AIML25\\mas\\ma2\\ma2\\.env\n",
      "API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from decouple import config, Config, RepositoryEnv\n",
    "\n",
    "# Get the absolute path to the .env file\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "print(\"Looking for .env file at:\", env_path)\n",
    "\n",
    "# Create a Config object with the explicit path\n",
    "config = Config(RepositoryEnv(env_path))\n",
    "\n",
    "# Try to load the API key\n",
    "try:\n",
    "    WX_API_KEY = config('WX_API_KEY')\n",
    "    print(\"API key loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading API key:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials=credentials, \n",
    "    project_id=\"292b3d04-e9e8-4874-a7f6-a1de426bde30\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-13b-instruct-v2',\n",
       " 'created_at': '2025-03-23T20:32:02.914Z',\n",
       " 'results': [{'generated_text': 'Mix the ingredients together in a bowl. Pour the batter into a cake pan. Bake for 30 minutes',\n",
       "   'generated_token_count': 20,\n",
       "   'input_token_count': 7,\n",
       "   'stop_reason': 'max_tokens'}],\n",
       " 'system': {'warnings': [{'message': \"The value of 'parameters.max_new_tokens' for this model was set to value 20\",\n",
       "    'id': 'unspecified_max_new_tokens',\n",
       "    'additional_properties': {'limit': 0,\n",
       "     'new_value': 20,\n",
       "     'parameter': 'parameters.max_new_tokens',\n",
       "     'value': 0}}]}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"How do I make a cake?\"\n",
    "generated_response = model.generate(prompt)\n",
    "\n",
    "generated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| PARAMETER             | TYPE                                   | EXAMPLE VALUE                                                                                                                             |\n",
      "+=======================+========================================+===========================================================================================================================================+\n",
      "| decoding_method       | str, TextGenDecodingMethod, NoneType   | sample                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| length_penalty        | dict, TextGenLengthPenalty, NoneType   | {'decay_factor': 2.5, 'start_index': 5}                                                                                                   |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| temperature           | float, NoneType                        | 0.5                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_p                 | float, NoneType                        | 0.2                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_k                 | int, NoneType                          | 1                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| random_seed           | int, NoneType                          | 33                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| repetition_penalty    | float, NoneType                        | 2                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| min_new_tokens        | int, NoneType                          | 50                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| max_new_tokens        | int, NoneType                          | 1000                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| stop_sequences        | list, NoneType                         | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| time_limit            | int, NoneType                          | 600000                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| truncate_input_tokens | int, NoneType                          | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| return_options        | dict, ReturnOptionProperties, NoneType | {'input_text': True, 'generated_tokens': True, 'input_tokens': True, 'token_logprobs': True, 'token_ranks': False, 'top_n_tokens': False} |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| include_stop_sequence | bool, NoneType                         | True                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| prompt_variables      | dict, NoneType                         | {'doc_type': 'emails', 'entity_name': 'Golden Retail'}                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters\n",
    "\n",
    "TextGenParameters.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0.8,      # Higher temperature means more randomness\n",
    "    max_new_tokens=500, # Maximum number of tokens to generate\n",
    "    min_new_tokens=200, # Minimum number of tokens to generate\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-13b-instruct-v2',\n",
       " 'created_at': '2025-03-23T20:32:08.874Z',\n",
       " 'results': [{'generated_text': 'To make a cake, you will need flour, baking powder, salt, eggs, milk, oil, and sugar. Preheat your oven to 350 degrees. Mix together the flour, baking powder, and salt in a large bowl. In a separate bowl, mix together the eggs, milk, oil, and sugar. Add the wet ingredients to the dry ingredients and mix together until well combined. Pour the batter into a greased and floured cake pan. Bake the cake for 30 to 40 minutes, or until a toothpick inserted in the center comes out clean. Let the cake cool before frosting. icing icing ideas : Vanilla Buttercream Icing icing ideas : Chocolate Buttercream Icing icing ideas : Cream Cheese Icing icing ideas : Strawberry Buttercream Icing icing ideas : Lemon Buttercream Icing icing ideas : Carrot Cake Icing icing ideas : Red Velvet Icing icing ideas : Banana Caramel Icing icing ideas : Blueberry Icing icing ideas : Cream Cheese Frosting icing ideas : Chocolate Frosting icing ideas : Peanut Butter Frosting icing ideas : White Chocolate Buttercream ',\n",
       "   'generated_token_count': 246,\n",
       "   'input_token_count': 7,\n",
       "   'stop_reason': 'eos_token',\n",
       "   'seed': 439699064}]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make a cake, you will need flour, baking powder, salt, eggs, milk, oil, and sugar. Preheat your oven to 350 degrees. Mix together the flour, baking powder, and salt in a large bowl. In a separate bowl, mix together the eggs, milk, oil, and sugar. Add the wet ingredients to the dry ingredients and mix together until well combined. Pour the batter into a greased and floured cake pan. Bake the cake for 30 to 40 minutes, or until a toothpick inserted in the center comes out clean. Let the cake cool before frosting. icing icing ideas : Vanilla Buttercream Icing icing ideas : Chocolate Buttercream Icing icing ideas : Cream Cheese Icing icing ideas : Strawberry Buttercream Icing icing ideas : Lemon Buttercream Icing icing ideas : Carrot Cake Icing icing ideas : Red Velvet Icing icing ideas : Banana Caramel Icing icing ideas : Blueberry Icing icing ideas : Cream Cheese Frosting icing ideas : Chocolate Frosting icing ideas : Peanut Butter Frosting icing ideas : White Chocolate Buttercream \n"
     ]
    }
   ],
   "source": [
    "print(response[\"results\"][0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frede\\AppData\\Local\\Temp\\ipykernel_19388\\4064009810.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(frac=frac, random_state=seed))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((760, 2),)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 1e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape, # train_df.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=10,          # Maximum number of tokens to generate\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # We could also try a larger model!\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a news classification expert. Let's classify this article with detailed context for each category:\n",
    "\n",
    "CATEGORY DEFINITIONS AND EXAMPLES:\n",
    "\n",
    "1. World News:\n",
    "   - International politics and diplomacy\n",
    "   - Global conflicts and peace\n",
    "   - International organizations (UN, WHO, etc.)\n",
    "   - Cross-border issues (climate change, migration)\n",
    "   Example: \"Global leaders meet at UN summit to discuss climate change policies\"\n",
    "   Example: \"International trade agreement signed between major economies\"\n",
    "\n",
    "2. Sports:\n",
    "   - Professional and amateur athletics\n",
    "   - Team and individual sports\n",
    "   - Tournaments and championships\n",
    "   - Sports organizations and leagues\n",
    "   Example: \"Local team wins championship after dramatic final match\"\n",
    "   Example: \"Olympic athlete breaks world record in swimming\"\n",
    "\n",
    "3. Business:\n",
    "   - Companies and corporations\n",
    "   - Financial markets and trading\n",
    "   - Economic indicators and trends\n",
    "   - Industry developments\n",
    "   Example: \"Tech company announces record quarterly earnings\"\n",
    "   Example: \"Stock market reaches new heights as investors show confidence\"\n",
    "\n",
    "4. Sci/Tech:\n",
    "   - Scientific discoveries and research\n",
    "   - Technological innovations\n",
    "   - Space exploration\n",
    "   - Medical breakthroughs\n",
    "   Example: \"Scientists discover new species in deep ocean\"\n",
    "   Example: \"Breakthrough in quantum computing research\"\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "IMPORTANT: After analyzing the article against these detailed categories, respond with ONLY ONE WORD - the category name.\n",
    "Example valid responses: \"Business\", \"Sports\", \"Sci/Tech\", \"World\"\n",
    "\n",
    "Category:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [04:18<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # format the prompt with the categories and the text\n",
    "    prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n",
    "    \n",
    "    # generate the response from the model\n",
    "    response = model.generate(prompt)\n",
    "\n",
    "    # extract the generated text from the response\n",
    "    prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "    # append the prediction to the list of predictions\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.63      0.90      0.74       190\n",
      "    Sci/Tech       0.69      0.64      0.66       190\n",
      "      Sports       0.82      0.94      0.88       190\n",
      "       World       0.91      0.46      0.61       190\n",
      "\n",
      "    accuracy                           0.73       760\n",
      "   macro avg       0.76      0.73      0.72       760\n",
      "weighted avg       0.76      0.73      0.72       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df.label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflections. \n",
    "I was unable to increase the performance of the LLLM beyond the bert and BoW models. I approached differnet system prompts, trying to give the LLM more context, and encourage chain of thought reasoning, different base models, but the performance was never quite impressive enough. I would imagine that this is an  ill suited task for an LLM, since the amount of mental arithmetic in compressing the entire article down to a single token (or atleast a single word) is quite  high. This should still be feasible somehow. Perhaps the BERT and BoW models perform better because they are more apt at calculating, and because they are more specalized for such a task,  compared to a more general purpose LLM. Prompt engineering was unsurprisingly incredibly important for performance, and I wound up using an LLM to generate a high context prompt for this purpose (something that I found is  extremely useful for improving LLM outputs, for example in deep research). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "    Business       0.63      0.90      0.74       190\n",
    "    Sci/Tech       0.69      0.64      0.66       190\n",
    "      Sports       0.82      0.94      0.88       190\n",
    "       World       0.91      0.46      0.61       190\n",
    "\n",
    "    accuracy                           0.73       760\n",
    "   macro avg       0.76      0.73      0.72       760\n",
    "weighted avg       0.76      0.73      0.72       760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
